---
title: "Lab 4 - Advanced Machine Learning"
author: "Simon Jorstedt"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
---

# Problem 2.1 - Algorithm implementation
## Subproblem 2.1.1
```{r}
# Covariance function
SquaredExpKernel <- function(x1, x2, sigmaF=3, l=.3){
  n1 <- length(x1)
  n2 <- length(x2)
  K <- matrix(NA,n1,n2)
  for (i in 1:n2){
    K[,i] <- sigmaF^2*exp(-0.5*( (x1-x2[i])/l)^2 )
  }
  return(K)
}
```

```{r}
posteriorGP <- function(X, y, XStar, sigmaNoise, k){
  # Returns a list of sublists (for each XStar value), each containing f_mean, Variance, loglikelihood.
  K <- k(X, X)
  L <- t(chol(K + sigmaNoise**2 * diag(length(X))))
  
  alpha <- solve(t(L), solve(L, y))
  
  #return(alpha)
  
  # Collect outputs for each XStar value
  f_mean_vec <- c()
  Var_vec <- c()
  #loglikelihood? no?
  
  for (i in 1:length(XStar)){
    k_star <- matrix(k(XStar[[i]], X),
                     nrow=length(X))
    
    f_star <- t(k_star) %*% alpha
    
    v <- solve(L, k_star)
    
    V_of_f_star <- k(XStar[[i]], XStar[[i]]) - t(v) %*% v
    
    # Save mean and variance
    f_mean_vec <- c(f_mean_vec, f_star)
    Var_vec <- c(Var_vec, V_of_f_star)
    #output[[i]] <- list(f_star, V_of_f_star, log_p_y_X)
  }
  
  log_p_y_X <- -1/2 * t(y) %*% alpha - sum(log(diag(L))) - length(X)/2*log(2*pi)
  
  return(list(means = f_mean_vec, variances = Var_vec, loglik=log_p_y_X))
}
```

## Subproblem 2.1.2 - Single datapoint

```{r}
# Plot figure 1
x_star_values <- seq(-1, 1, 0.01)
result_1 <- posteriorGP(X=c(0.4),
                        y=c(0.719),
                        XStar=x_star_values,
                        sigmaNoise=0.1,
                        k=SquaredExpKernel)

lower_cfi_curve <- qnorm(p=0.025, mean=result_1$means, sd = result_1$variances**(1/2))
upper_cfi_curve <- qnorm(p=0.975, mean=result_1$means, sd = result_1$variances**(1/2))

plot(x_star_values,
     result_1$means,
     type="l",
     ylim=range(lower_cfi_curve, 
                upper_cfi_curve), 
     main="Fig 1: Observation (x,y) = (0.4, 0.719)",
     xlab="XStar",
     ylab="Mean")
points(c(0.4), c(0.719))
points(x_star_values, lower_cfi_curve, type="l", col="darkblue")
points(x_star_values, upper_cfi_curve, type="l", col="darkblue")
```

## Subproblem 2.1.3 - Additional datapoint

```{r}
# Plot figure 2
result_2 <- posteriorGP(X=c(0.4, -0.6),
                        y=c(0.719, -0.044),
                        XStar=x_star_values,
                        sigmaNoise=0.1,
                        k=SquaredExpKernel)

lower_cfi_curve_2 <- qnorm(p=0.025, mean=result_2$means, sd = result_2$variances**(1/2))
upper_cfi_curve_2 <- qnorm(p=0.975, mean=result_2$means, sd = result_2$variances**(1/2))

plot(x_star_values,
     result_2$means,
     type="l",
     ylim=range(lower_cfi_curve_2, 
                upper_cfi_curve_2), 
     main="Fig 2: Two observations",
     xlab="XStar",
     ylab="Mean")
points(x_star_values, lower_cfi_curve_2, type="l", col="darkblue")
points(x_star_values, upper_cfi_curve_2, type="l", col="darkblue")
points(c(0.4, -0.6), c(0.719, -0.044))
```

## Subproblem 2.1.4 - Five datapoints

```{r}
# Plot figure 3
result_3 <- posteriorGP(X=c(-1, -0.6, -0.2, 0.4, 0.8),
                        y=c(0.768, -0.044, -0.940, 0.719, -0.664),
                        XStar=x_star_values,
                        sigmaNoise=0.1,
                        k=SquaredExpKernel)

lower_cfi_curve_3 <- qnorm(p=0.025, mean=result_3$means, sd = result_3$variances**(1/2))
upper_cfi_curve_3 <- qnorm(p=0.975, mean=result_3$means, sd = result_3$variances**(1/2))

plot(x_star_values,
     result_3$means,
     type="l",
     ylim=range(lower_cfi_curve_3, 
                upper_cfi_curve_3), 
     main="Fig 3: Five observations",
     xlab="XStar",
     ylab="Mean")
points(x_star_values, lower_cfi_curve_3, type="l", col="darkblue")
points(x_star_values, upper_cfi_curve_3, type="l", col="darkblue")
points(c(-1, -0.6, -0.2, 0.4, 0.8), c(0.768, -0.044, -0.940, 0.719, -0.664))
```

## Subproblem 2.1.5 - Five datapoints (sigmaF=1, l=1)
Below in Figure 4 we again fit a model on the five datapoints fro above, but this time with hyperparameters sigmaf=1 and l=1. They are clearly a bad choice of hyperparameters, as the curve does not quite capture the data. The fit in Figure 3 is much better, sine it intersects with the data and is quite smooth.

```{r}
# Plot figure 4

# Create SquaredExpKernel with different hyperparameters
alt_SquaredExpKernel <- SquaredExpKernel
formals(alt_SquaredExpKernel)$sigmaF = 1
formals(alt_SquaredExpKernel)$l = 1

result_4 <- posteriorGP(X=c(-1, -0.6, -0.2, 0.4, 0.8),
                        y=c(0.768, -0.044, -0.940, 0.719, -0.664),
                        XStar=x_star_values,
                        sigmaNoise=0.1,
                        k=alt_SquaredExpKernel)

lower_cfi_curve_4 <- qnorm(p=0.025, mean=result_4$means, sd = result_4$variances**(1/2))
upper_cfi_curve_4 <- qnorm(p=0.975, mean=result_4$means, sd = result_4$variances**(1/2))

plot(x_star_values,
     result_4$means,
     type="l",
     ylim=range(lower_cfi_curve_4, 
                upper_cfi_curve_4), 
     main="Fig 4: Five observations (sigma_f=1, l=1)",
     xlab="XStar",
     ylab="Mean")
points(x_star_values, lower_cfi_curve_4, type="l", col="darkblue")
points(x_star_values, upper_cfi_curve_4, type="l", col="darkblue")
points(c(-1, -0.6, -0.2, 0.4, 0.8), c(0.768, -0.044, -0.940, 0.719, -0.664))
```

# Problem 2.2 - GP Regression with Kernlab
We will now work with some data documenting the mean temperature in Tullinge, Stockholm for a period of six years.

```{r}
# Read data
data_temp <- read.csv("https://github.com/STIMALiU/AdvMLCourse/raw/master/GaussianProcess/Code/TempTullinge.csv", header=TRUE, sep=";")

data_temp$time <- as.numeric(1:2190)
data_temp$day <- as.numeric(rep(1:365, 6))

data_temp_subset <- data_temp[seq(1, nrow(data_temp), 5),]

library(kernlab)
```

## Subproblem 2.2.1
Below we define our own square exponential kernel and evaluate it in the point $(1,2)$, and then we calculate the covariance matrix $K(X,X_\star)$ for the input vectors given below.

```{r}
# Specify hyperparams
sigmaf <- 20
ell <- 100
SEkernel <- rbfdot(sigma = 1/(2*ell^2))

# Evaluated at (1,2)
SEkernel(1,2)

#
kernelMatrix(SEkernel, x=c(1,3,4), y=c(2,3,4))
```

## Subproblem 2.2.2
Now we consider a Gaussian process model with time as an explanatory variable for the temperature. Below we see this data overlaid with the predicted values from the fitted model, along with 95% probability bands.

```{r}
# Estimate sigma_n
polyFit <- lm(temp ~ time + time**2, data=data_temp_subset)
sigma_n_squared = sd(polyFit$residuals)**2

gaupr_res_1 <- gausspr(x=data_temp_subset$time, y=data_temp_subset$temp,
                       kernel = SEkernel,
                       kpar = list(sigma, ell),
                       var = sigma_n_squared,
                       variance.model = TRUE,
                       scaled=FALSE)

predicted_1 <- predict(gaupr_res_1, data_temp_subset$time)

# Plot 5
plot(data_temp_subset$time, data_temp_subset$temp, t="l",
     main="Fig 5: gausspr approximation (time)",
     xlab="time",
     ylab="temperature")
points(data_temp_subset$time, predicted_1, t="l", col="red")
points(data_temp_subset$time,
       predicted_1+predict(gaupr_res_1, data_temp_subset$time, type="sdeviation"),
       t="l", col="blue")
points(data_temp_subset$time,
       predicted_1-predict(gaupr_res_1, data_temp_subset$time, type="sdeviation"),
       t="l", col="blue")
```

## Subproblem 2.2.3
Below we again train the model, but this time using our implementation of algorithm 2.1 from Rasmussen and Williams' book. Below we see the result of this, similarly to the previous plot.

```{r}
# Plot figure 6

# Create SquaredExpKernel with different hyperparameters
alt_SquaredExpKernel <- SquaredExpKernel
formals(alt_SquaredExpKernel)$sigmaF = 20
formals(alt_SquaredExpKernel)$l = 100

result_5 <- posteriorGP(X=data_temp_subset$time,
                        y=data_temp_subset$temp,
                        XStar=data_temp_subset$time,
                        sigmaNoise=sigma_n_squared**(.5),
                        k=alt_SquaredExpKernel)

lower_cfi_curve_5 <- qnorm(p=0.025, mean=result_5$means, sd = result_5$variances**(1/2))
upper_cfi_curve_5 <- qnorm(p=0.975, mean=result_5$means, sd = result_5$variances**(1/2))

plot(data_temp_subset$time,
     result_5$means,
     type="l",
     ylim=range(data_temp_subset$temp), 
     main="Fig 6: posteriorGP approximation (time)", col="red",
     xlab="time",
     ylab="temperature")
points(data_temp_subset$time, lower_cfi_curve_5, type="l", col="darkblue")
points(data_temp_subset$time, upper_cfi_curve_5, type="l", col="darkblue")
points(data_temp_subset$time, data_temp_subset$temp, type="l")
```

## Subproblem 2.2.4
Now we train a model where the temperature is explained by the day variable. We choose not to superimpose the mean of this model with the mean of the model with time as explanatory variable because this makes a quite cluttered plot. However they can be compared by comparing the plots.

```{r}
# Estimate sigma_n
polyFit_2 <- lm(temp ~ day + day**2, data=data_temp_subset)
sigma_n_squared_2 = sd(polyFit_2$residuals)**2

gaupr_res_2 <- gausspr(x=data_temp_subset$day,
                       y=data_temp_subset$temp,
                       kernel = SEkernel,
                       kpar = list(sigma, ell),
                       var = sigma_n_squared_2,
                       variance.model = TRUE,
                       scaled=FALSE)

predicted_2 <- predict(gaupr_res_2, data_temp_subset$day)

# Plot 7
plot(data_temp_subset$time, data_temp_subset$temp, t="l",
     main="Fig 7: gausspr approximation (day)",
     xlab="time",
     ylab="temperature")
points(data_temp_subset$time, predicted_2, t="l", col="red")
points(data_temp_subset$time,
       predicted_2+predict(gaupr_res_2, data_temp_subset$day, type="sdeviation"),
       t="l", col="blue")
points(data_temp_subset$time,
       predicted_2-predict(gaupr_res_2, data_temp_subset$day, type="sdeviation"),
       t="l", col="blue")
```

## Subproblem 2.2.5 - Periodic kernel
Now we implement a periodic kernel and use it to fit a model to our problem, using the time variable. The result is quite a good fitted model that follows the periodic temperature changes quite well.

```{r}
# Hyperparameters
sigmaf <- 20
ell_1 <- 1
ell_2 <- 100

# Define periodic kernel
periodic_kernel <- function(x1, x2, sigmaf=20, ell_1=1, ell_2=100){
  d <- 365
  exp1 <- exp(-2/ell_1^2 * sin(pi*abs(x1-x2)/d)**2)
  exp2 <- exp(-0.5*( (x1-x2)/ell_2)^2 )
  
  k <- sigmaf**2 * exp1 * exp2
  
  return(k)
}
class(periodic_kernel) <- "kernel"
```

```{r}
# Estimate sigma_n
polyFit_6 <- lm(temp ~ time + time**2, data=data_temp_subset)
sigma_n_squared_2 = sd(polyFit_6$residuals)**2

gaupr_res_6 <- gausspr(x=data_temp_subset$time,
                       y=data_temp_subset$temp,
                       kernel = periodic_kernel,
                       kpar = list(sigmaf, ell_1, ell_2),
                       var = sigma_n_squared_2,
                       variance.model = TRUE,
                       scaled=FALSE)

predicted_6 <- predict(gaupr_res_6, data_temp_subset$time)

# Plot 8
plot(data_temp_subset$time, data_temp_subset$temp, t="l",
     main="Fig 8: gausspr approximation (time) (periodic kernel)",
     xlab="time",
     ylab="temperature")
points(data_temp_subset$time, predicted_6, t="l", col="red")
points(data_temp_subset$time,
       predicted_6+predict(gaupr_res_6, data_temp_subset$time, type="sdeviation"),
       t="l", col="blue")
points(data_temp_subset$time,
       predicted_6-predict(gaupr_res_6, data_temp_subset$time, type="sdeviation"),
       t="l", col="blue")
```

# Problem 2.3 - GP Classification with kernlab
## Subproblem 2.3.1
```{r}
# Read data
data <- read.csv("https://github.com/STIMALiU/AdvMLCourse/raw/master/GaussianProcess/Code/banknoteFraud.csv", header=FALSE, sep=",")
names(data) <- c("varWave","skewWave","kurtWave","entropyWave","fraud")
data[,5] <- as.factor(data[,5])
 
set.seed(111); SelectTraining <- sample(1:dim(data)[1], size = 1000, replace = FALSE)

data_train <- data[SelectTraining,]
```

```{r, out.width="70%"}
# Fit Gaussian process
gaupr_res_7 <- gausspr(x=data_train[,c(1,2)],
                       y=data_train$fraud)

varWave_grid <- seq(min(data_train$varWave),
                    max(data_train$varWave),
                    length.out=100)
skewWave_grid <- seq(min(data_train$skewWave),
                    max(data_train$skewWave),
                    length.out=100)

# For meshgrid
library(pracma)
meshgrid_var_skew <- meshgrid(varWave_grid, skewWave_grid)
contour_df <- cbind(c(meshgrid_var_skew$X), c(meshgrid_var_skew$Y))

predicted_probs <- predict(gaupr_res_7, contour_df, type="probabilities")

# Plot 9
contour(x=varWave_grid, y=skewWave_grid,
        z=matrix(predicted_probs[,2], nrow=100, ncol=100),
        xlab="varWave",
        ylab="skewWave",
        main="Fig 9: Contour plot of positive(1) probability,\npositive (red) and negative (blue) samples included.")
points(data_train[data_train$fraud == 0,c(1,2)], col="blue")
points(data_train[data_train$fraud == 1,c(1,2)], col="red")
```

## Subproblem 2.3.2

```{r}
# Predict on train data
fraud_predictions <- predict(gaupr_res_7, data_train[,c(1,2)])

# Confusion matrix and accuracy
library(caret)
print("Confusion matrix for train data")
confusionMatrix(fraud_predictions, data_train$fraud)

# Test data confusion matrix and accuracy
fraud_predictions_testdata <- predict(gaupr_res_7, data[-SelectTraining,][,c(1,2)])
print("Confusion matrix for test data")
confusionMatrix(fraud_predictions_testdata, data[-SelectTraining,]$fraud)
```

## Subproblem 2.3.3
Below we see the result of training the model on all four covariates, and it appears evident that the model certainly achieves a higher accuracy (~99%) on the test data, than when only the first two covariates were used previously (~92%). In particular, the respective accuracy confidence intervals do not overlap, supporting this.

```{r}
# Fit Gaussian process
gaupr_res_8 <- gausspr(x=data_train[,c(1,2,3,4)],
                       y=data_train$fraud)

# Test data confusion matrix and accuracy
fraud_predictions_testdata_2 <- predict(gaupr_res_8, data[-SelectTraining,][,c(1,2,3,4)])
print("Confusion matrix for test data")
confusionMatrix(fraud_predictions_testdata_2, data[-SelectTraining,]$fraud)
```

