---
title: "Lab 1 Marijn"
author: "Marijn Jaarsma"
output: pdf_document
date: "2024-09-10"
---

```{r Setup, include=FALSE}
# Install required packages
# if (!requireNamespace("BiocManager", quietly=TRUE))
# install.packages("BiocManager")
# 
# BiocManager::install("RBGL")
# BiocManager::install("Rgraphviz")
# BiocManager::install("gRain", force=TRUE)
```


```{r Libraries, include=FALSE}
library(gRain)
library(bnlearn)
library(dplyr)
library(caret)
```

# Question 1
```{r}
# Load data
data("asia")

# Create and compare graphs with HC alg
graph1 <- hc(asia, score="bde", iss=100, restart=10)
graph2 <- hc(asia, score="bde", iss=1, restart=10)

graphviz.compare(graph1, graph2)
```

The HC algorithm may find different network structures because it is not guaranteed to find a global optimum. In the comparison above, the imaginary sample size (ISS) was changed which means that one network regularizes less with the Bayesian score. This network is much bigger, with many more edges than the network with small ISS. Increasing the number of random restarts may also result in different graphs as introducing more randomness may lead to separate runs of the algorithm to find a different local optimum. 

# Question 2
```{r}
# Sample 80% of data for training
sample_ind <- sample(1:nrow(asia), 0.8 * nrow(asia))
df_train <- asia[sample_ind,]
df_test <- asia[-sample_ind,]

# Learn structure and parameters
# https://www.bnlearn.com/examples/fit/
graph <- hc(df_train, score="bde", iss=3, restart=20)
param <- bn.fit(graph, df_train, method="bayes")

# Visualize and compare to true model
# graphviz.plot(graph)
print(param)
# bn.fit.barchart(fitted_graph$S)

dag <- model2network("[A][S][T|A][L|S][B|S][D|B:E][E|T:L][X|E]")
graphviz.compare(graph, bn.fit(dag, asia))
```

```{r}
# Convert to grain
grain <- compile(as.grain(param))

pred <- rep(0, nrow(df_test))
nodes <- names(df_test)[!names(df_test) %in% "S"]
for (i in 1:nrow(df_test)) {
  # Record evidence
  states <- as.vector(t(df_test[i, nodes]))
  
  # # https://www.rdocumentation.org/packages/gRain/versions/1.3-2/topics/grain-evidence
  evidence <- setEvidence(grain, nodes, states)
  
  # https://www.rdocumentation.org/packages/gRain/versions/1.4.1/topics/querygrain
  pred[i] <- names(which.max(querygrain(evidence, "S", evidence=evidence)$S))

}

# Compute confusion matrix
confusionMatrix(factor(pred), factor(df_test$S))
```

# Question 3
```{r}
pred <- rep(0, nrow(df_test))
for (i in 1:nrow(df_test)) {
  # Record evidence
  nodes <- mb(param, "S")
  states <- as.vector(t(df_test[i, nodes]))
  
  # # https://www.rdocumentation.org/packages/gRain/versions/1.3-2/topics/grain-evidence
  evidence <- setEvidence(grain, nodes, states)
  
  # https://www.rdocumentation.org/packages/gRain/versions/1.4.1/topics/querygrain
  # pred[i] <- querygrain(grain, "S", evidence=evidence)
  pred[i] <- names(which.max(querygrain(evidence, "S")$S))
}

confusionMatrix(factor(pred), factor(df_test$S))
```

# Question 4
```{r}
# Create naive Bayesian graph
# https://www.bnlearn.com/examples/dag/
graph <- empty.graph(c("A", "S", "T", "L", "B", "D", "E", "X"))
arc_set <- matrix(c("S", "A", "S", "T", "S", "L", "S", "B", "S", "D", "S", "E", "S", "X"), 
                  ncol=2, byrow=TRUE, dimnames=list(NULL, c("from", "to")))
arcs(graph) <- arc_set

graphviz.plot(graph)
```

```{r}
# Train and convert to grain
param <- bn.fit(graph, df_train, method="bayes")
grain <- compile(as.grain(param))

pred <- rep(0, nrow(df_test))
nodes <- names(df_test)[!names(df_test) %in% "S"]
for (i in 1:nrow(df_test)) {
  # Record evidence
  states <- as.vector(t(df_test[i, nodes]))
  
  # # https://www.rdocumentation.org/packages/gRain/versions/1.3-2/topics/grain-evidence
  evidence <- setEvidence(grain, nodes, states)
  
  # https://www.rdocumentation.org/packages/gRain/versions/1.4.1/topics/querygrain
  pred[i] <- names(which.max(querygrain(evidence, "S", evidence=evidence)$S))

}

# Compute confusion matrix
confusionMatrix(factor(pred), factor(df_test$S))
```

# Question 5
Markov blanket is the minimal set of nodes that separates a given node from the rest. In question 2 and question 3, there was no difference in accuracy between using the full model as evidence and using the Markov blanket, as the Markov blanket retains all necessary information to do inference on S. The naive classifier performs a little bit worse than the other two, likely because the model is modeling dependencies incorrectly. This may lead to other variables impacting S within this graph, while that is not true in reality.