theta <- sample_proposal
}
sample[i,] <- theta
}
return(sample)
}
# Get sample and visualize
sample <- metropolis_hastings(init_val, 1000, log_post_poisson, X, y, solve(-optim_res$hessian), 1)
for (col in 1:ncol(sample)) {
hist(sample[, col], breaks=20, main=paste0("Beta of ", colnames(X)[col]))
}
sample
x %*% t(sample[1,])
x
sample[1,]
t(sample[1,])
x %*% t(sample[1,])
t(x) %*% t(sample[1,])
sample[1,]
sample[1,] %*% x
sample[1,] %*% x
sample %*% x
dpois(1:100, sample %*% x)
hist(dpois(1:100, sample %*% x))
y
probs <- dpois(seq(1:20, 0.1), sample %*% x)
probs <- dpois(seq(1, 20, 0.1), sample %*% x)
seq(1, 20, 0.1)
hist(probs)
x <- c(1, 1, 0, 1, 0, 1, 0, 1.2, 0.8)
probs <- dpois(seq(0, 20, 0.1), sample %*% x)
hist(probs)
# Function to draw samples using MH algorithm
metropolis_hastings <- function(theta, n_samples, log_post_fun, X, y, Sigma, const) {
sample <- matrix(nrow=n_samples, ncol=nrow(theta))
for (i in 1:n_samples) {
sample_proposal <- t(rmvnorm(1, theta, const * Sigma))
mh_ratio <- exp(log_post_fun(sample_proposal, X, y) - log_post_fun(theta, X, y))
acc_prob <- min(1, mh_ratio)
# If accepted add proposal to sample, else add previous to sample
if (runif(1) <= acc_prob) {
theta <- sample_proposal
}
sample[i,] <- theta
}
return(sample)
}
# Get sample and visualize
sample <- metropolis_hastings(init_val, 1000, log_post_poisson, X, y, solve(-optim_res$hessian), 1)
for (col in 1:ncol(sample)) {
plot(sample[, col], main=paste0("Beta of ", colnames(X)[col]))
}
# Function to draw samples using MH algorithm
metropolis_hastings <- function(theta, n_samples, log_post_fun, X, y, Sigma, const) {
sample <- matrix(nrow=n_samples, ncol=nrow(theta))
for (i in 1:n_samples) {
sample_proposal <- t(rmvnorm(1, theta, const * Sigma))
mh_ratio <- exp(log_post_fun(sample_proposal, X, y) - log_post_fun(theta, X, y))
acc_prob <- min(1, mh_ratio)
# If accepted add proposal to sample, else add previous to sample
if (runif(1) <= acc_prob) {
theta <- sample_proposal
}
sample[i,] <- theta
}
return(sample)
}
# Get sample and visualize
sample <- metropolis_hastings(init_val, 1000, log_post_poisson, X, y, solve(-optim_res$hessian), 1)
for (col in 1:ncol(sample)) {
plot(sample[, col], type="l", main=paste0("Beta of ", colnames(X)[col]))
}
# Function to draw samples using MH algorithm
metropolis_hastings <- function(theta, n_samples, log_post_fun, X, y, Sigma, const) {
sample <- matrix(nrow=n_samples, ncol=nrow(theta))
for (i in 1:n_samples) {
sample_proposal <- t(rmvnorm(1, theta, const * Sigma))
mh_ratio <- exp(log_post_fun(sample_proposal, X, y) - log_post_fun(theta, X, y))
acc_prob <- min(1, mh_ratio)
# If accepted add proposal to sample, else add previous to sample
if (runif(1) <= acc_prob) {
theta <- sample_proposal
}
sample[i,] <- theta
}
return(sample)
}
# Get sample and visualize
sample <- metropolis_hastings(init_val, 2000, log_post_poisson, X, y, solve(-optim_res$hessian), 1)
for (col in 1:ncol(sample)) {
plot(sample[, col], type="l", main=paste0("Beta of ", colnames(X)[col]))
}
knitr::opts_chunk$set(echo = TRUE)
# Define function for the log posterior
log_post <- function(theta, sum_xi, n) {
log_lik <- sum_xi * log(theta) - n * theta
log_prior <- 5 * log(theta) - 2 * theta
return(log_lik + log_prior)
}
# Values from question
sum_xi <- 63
n <- 14
theta <- seq(from=0, to=10, by=0.01)
# Compute posterior
dist <- exp(log_post(theta, sum_xi, n))
dist <- dist / (0.01 * sum(dist))
# Plot posterior
plot(theta, dist, main="Posterior Distribution", xlab="theta", ylab="p")
# Perform numerical optimisation
res <- optim(0, log_post, gr=NULL, sum_xi, n, method=c("L-BFGS-B"), lower=2, control=list(fnscale=-1), hessian=TRUE)
# Plot versus true posterior
plot(theta, dist, main="Posterior Distribution", xlab="theta", ylab="p")
lines(theta, dnorm(theta, mean=res$par, sd=sqrt(-1 / res$hessian)), col="red", type="p")
n_sim <- 1000
sample <- rep(0, n_sim)
for (i in 1:n_sim) {
theta <- rgamma(1, shape=6 + sum_xi, rate=2 + n)
x <- rpois(14, theta)
sample[i] <- max(x)
}
hist(sample)
p <- length(sample[sample >= 7]) / length(sample)
print(p)
library(mvtnorm)
# Load in data
load("Summerhouses.RData")
# Provided function
BayesLinReg <- function(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter){
# Direct sampling from a Gaussian linear regression with conjugate prior:
#
# beta | sigma2 ~ N(mu_0, sigma2*inv(Omega_0))
# sigma2 ~ Inv-Chi2(v_0,sigma2_0)
#
# INPUTS:
#   y - n-by-1 vector with response data observations
#   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
#   mu_0 - prior mean for beta
#   Omega_0  - prior precision matrix for beta
#   v_0      - degrees of freedom in the prior for sigma2
#   sigma2_0 - location ("best guess") in the prior for sigma2
#   nIter - Number of samples from the posterior (iterations)
#
# OUTPUTS:
#   results$betaSample     - Posterior sample of beta.     nIter-by-nCovs matrix
#   results$sigma2Sample   - Posterior sample of sigma2.   nIter-by-1 vector
# Compute posterior hyperparameters
n = length(y) # Number of observations
nCovs = dim(X)[2] # Number of covariates
XX = t(X)%*%X
betaHat <- solve(XX,t(X)%*%y)
Omega_n = XX + Omega_0
mu_n = solve(Omega_n,XX%*%betaHat+Omega_0%*%mu_0)
v_n = v_0 + n
sigma2_n = as.numeric((v_0*sigma2_0 + ( t(y)%*%y + t(mu_0)%*%Omega_0%*%mu_0 - t(mu_n)%*%Omega_n%*%mu_n))/v_n)
invOmega_n = solve(Omega_n)
# The actual sampling
sigma2Sample = rep(NA, nIter)
betaSample = matrix(NA, nIter, nCovs)
for (i in 1:nIter){
# Simulate from p(sigma2 | y, X)
sigma2 = (v_n*sigma2_n)/rchisq(n=1,df=v_n)
sigma2Sample[i] = sigma2
# Simulate from p(beta | sigma2, y, X)
beta_ = rmvnorm(n=1, mean = mu_n, sigma = sigma2*invOmega_n)
betaSample[i,] = beta_
}
return(results = list(sigma2Sample = sigma2Sample, betaSample=betaSample))
}
# Setup given parameters
mu_0 <- rep(0, 7)
Omega_0 <- 1 / (4 ** 2) * diag(7)
v_0 <- 1
sigma2_0 <- 2 ** 2
nIter <- 10000
# Draw sample
sample <- BayesLinReg(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter)
b1_b5 <- sample$betaSample[1:nrow(sample$betaSample), 2] + sample$betaSample[1:nrow(sample$betaSample), 6]
# Compute and print 95% confidence interval
cat(mean(b1_b5) - 1.96 * sd(b1_b5), mean(b1_b5) + 1.96 * sd(b1_b5))
xA <- matrix(c(1, 0.5, 0.7, 1, 0, 0.5, 0))
xB <- matrix(c(1, 0.5, 0.7, 0, 1, 0, 0.5))
hist(sample$betaSample %*% xA - sample$betaSample %*% xB)
?density
density(sample$betaSample %*% xA - sample$betaSample %*% xB)
plot(density(sample$betaSample %*% xA - sample$betaSample %*% xB))
plot(density(sample$betaSample %*% xA - sample$betaSample %*% xC))
xA <- matrix(c(1, 0.5, 0.7, 1, 0, 0.5, 0))
xB <- matrix(c(1, 0.5, 0.7, 0, 1, 0, 0.5))
plot(density(sample$betaSample %*% xA - sample$betaSample %*% xB))
xC <- matrix(c(1, 0.5, 0.7, 0, 0, 0, 0))
plot(density(sample$betaSample %*% xA - sample$betaSample %*% xC))
plot(density(sample$betaSample %*% x))
x <- matrix(c(1, -0.5, -0.3, 0, 1, 0, 0.5))
plot(density(sample$betaSample %*% x))
x <- matrix(c(1, -0.5, -0.3, 0, 1, 0, 0.5))
pred_x <- sample$betaSample %*% x
plot(density(pred_x))
# Compute predictions
x <- matrix(c(1, -0.5, -0.3, 0, 1, 0, 0.5))
pred_x <- sample$betaSample %*% x
# Plot distribution
plot(density(pred_x))
# Compute probability mu > -0.8
length(pred_x > -0.8) / length(pred_x)
length(pred_x > -0.8)
pred_x > -0.8
# Compute probability mu > -0.8
length(pred_x[pred_x > -0.8]) / length(pred_x)
# Compute probability mu > -0.8
cat(length(pred_x[pred_x > -0.8]) / length(pred_x))
library(mvtnorm)
# Load in data
load("Summerhouses.RData")
# Provided function
BayesLinReg <- function(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter){
# Direct sampling from a Gaussian linear regression with conjugate prior:
#
# beta | sigma2 ~ N(mu_0, sigma2*inv(Omega_0))
# sigma2 ~ Inv-Chi2(v_0,sigma2_0)
#
# INPUTS:
#   y - n-by-1 vector with response data observations
#   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
#   mu_0 - prior mean for beta
#   Omega_0  - prior precision matrix for beta
#   v_0      - degrees of freedom in the prior for sigma2
#   sigma2_0 - location ("best guess") in the prior for sigma2
#   nIter - Number of samples from the posterior (iterations)
#
# OUTPUTS:
#   results$betaSample     - Posterior sample of beta.     nIter-by-nCovs matrix
#   results$sigma2Sample   - Posterior sample of sigma2.   nIter-by-1 vector
# Compute posterior hyperparameters
n = length(y) # Number of observations
nCovs = dim(X)[2] # Number of covariates
XX = t(X)%*%X
betaHat <- solve(XX,t(X)%*%y)
Omega_n = XX + Omega_0
mu_n = solve(Omega_n,XX%*%betaHat+Omega_0%*%mu_0)
v_n = v_0 + n
sigma2_n = as.numeric((v_0*sigma2_0 + ( t(y)%*%y + t(mu_0)%*%Omega_0%*%mu_0 - t(mu_n)%*%Omega_n%*%mu_n))/v_n)
invOmega_n = solve(Omega_n)
# The actual sampling
sigma2Sample = rep(NA, nIter)
betaSample = matrix(NA, nIter, nCovs)
for (i in 1:nIter){
# Simulate from p(sigma2 | y, X)
sigma2 = (v_n*sigma2_n)/rchisq(n=1,df=v_n)
sigma2Sample[i] = sigma2
# Simulate from p(beta | sigma2, y, X)
beta_ = rmvnorm(n=1, mean = mu_n, sigma = sigma2*invOmega_n)
betaSample[i,] = beta_
}
return(results = list(sigma2Sample = sigma2Sample, betaSample=betaSample))
}
library(mvtnorm)
# Load in data
load("Summerhouses.RData")
# Provided function
BayesLinReg <- function(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter){
# Direct sampling from a Gaussian linear regression with conjugate prior:
#
# beta | sigma2 ~ N(mu_0, sigma2*inv(Omega_0))
# sigma2 ~ Inv-Chi2(v_0,sigma2_0)
#
# INPUTS:
#   y - n-by-1 vector with response data observations
#   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
#   mu_0 - prior mean for beta
#   Omega_0  - prior precision matrix for beta
#   v_0      - degrees of freedom in the prior for sigma2
#   sigma2_0 - location ("best guess") in the prior for sigma2
#   nIter - Number of samples from the posterior (iterations)
#
# OUTPUTS:
#   results$betaSample     - Posterior sample of beta.     nIter-by-nCovs matrix
#   results$sigma2Sample   - Posterior sample of sigma2.   nIter-by-1 vector
# Compute posterior hyperparameters
n = length(y) # Number of observations
nCovs = dim(X)[2] # Number of covariates
XX = t(X)%*%X
betaHat <- solve(XX,t(X)%*%y)
Omega_n = XX + Omega_0
mu_n = solve(Omega_n,XX%*%betaHat+Omega_0%*%mu_0)
v_n = v_0 + n
sigma2_n = as.numeric((v_0*sigma2_0 + ( t(y)%*%y + t(mu_0)%*%Omega_0%*%mu_0 - t(mu_n)%*%Omega_n%*%mu_n))/v_n)
invOmega_n = solve(Omega_n)
# The actual sampling
sigma2Sample = rep(NA, nIter)
betaSample = matrix(NA, nIter, nCovs)
for (i in 1:nIter){
# Simulate from p(sigma2 | y, X)
sigma2 = (v_n*sigma2_n)/rchisq(n=1,df=v_n)
sigma2Sample[i] = sigma2
# Simulate from p(beta | sigma2, y, X)
beta_ = rmvnorm(n=1, mean = mu_n, sigma = sigma2*invOmega_n)
betaSample[i,] = beta_
}
return(results = list(sigma2Sample = sigma2Sample, betaSample=betaSample))
}
# Setup given parameters
mu_0 <- rep(0, 7)
Omega_0 <- 1 / (4 ** 2) * diag(7)
v_0 <- 1
sigma2_0 <- 2 ** 2
nIter <- 10000
# Draw sample
sample <- BayesLinReg(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter)
b1_b5 <- sample$betaSample[1:nrow(sample$betaSample), 2] + sample$betaSample[1:nrow(sample$betaSample), 6]
# Compute and print 95% confidence interval
cat(mean(b1_b5) - 1.96 * sd(b1_b5), mean(b1_b5) + 1.96 * sd(b1_b5))
# Compute predictions for region A and B
xA <- matrix(c(1, 0.5, 0.7, 1, 0, 0.5, 0))
xB <- matrix(c(1, 0.5, 0.7, 0, 1, 0, 0.5))
pred_A <- sample$betaSample %*% xA
pred_B <- sample$betaSample %*% xB
# Plot difference in predictions
plot(density(pred_A - pred_B))
# Compute prediction for other regions
xC <- matrix(c(1, 0.5, 0.7, 0, 0, 0, 0))
pred_C <- sample$betaSample %*% xC
# Plot difference in predictions
plot(density(pred_A - pred_C))
# Compute predictions
x <- matrix(c(1, -0.5, -0.3, 0, 1, 0, 0.5))
pred_x <- sample$betaSample %*% x
# Plot distribution
plot(density(pred_x))
# Compute probability mu > -0.8
cat(length(pred_x[pred_x > -0.8]) / length(pred_x))
seq(min(X[1:nrow(X), 2]), max(X[1:nrow(X), 2]), 0.01)
x1_grid <- seq(min(X[1:nrow(X), 2]), max(X[1:nrow(X), 2]), 0.01)
for (i in 1:length(x1_grid)) {
sample$betaSample %*% x1_grid[i]
}
x1_grid[i]
sample$betaSample %*% c(1, x1_grid[i], 0.7, 1, 0, 0.5, 0)
?dmvnorm
sample$sigma2Sample
rmvnorm(nIter, mu, sample$sigma2Sample)
mu <- sample$betaSample %*% c(1, x1_grid[i], 0.7, 1, 0, 0.5, 0)
rmvnorm(nIter, mu, sample$sigma2Sample)
rmvnorm(nIter, 0, sample$sigma2Sample)
rnorm(nIter, 0, sample$sigma2Sample)
mu + eps
eps <- rnorm(nIter, 0, sample$sigma2Sample)
mu + eps
eps <- rnorm(nIter, mu, sample$sigma2Sample)
mu + eps
draw <- rnorm(nIter, mu, sample$sigma2Sample)
draw
eps <- rnorm(nIter, 0, sample$sigma2Sample)
mu + eps
rnorm(nIter, 0, sample$sigma2Sample) + mu
rnorm(nIter, 0, sample$sigma2Sample) + mu - rnorm(nIter, mu, sample$sigma2Sample)
x1_grid <- seq(min(X[1:nrow(X), 2]), max(X[1:nrow(X), 2]), 0.01)
for (i in 1:length(x1_grid)) {
mu <- sample$betaSample %*% c(1, x1_grid[i], 0.7, 1, 0, 0.5, 0)
eps <- rnorm(nIter, 0, sample$sigma2Sample)
mu + eps
}
hist(rnorm(nIter, 0, sample$sigma2Sample) + mu)
lines(density(rnorm(nIter, mu, sample$sigma2Sample)))
hist(rnorm(nIter, mu, sample$sigma2Sample))
x1_grid <- seq(min(X[1:nrow(X), 2]), max(X[1:nrow(X), 2]), 0.01)
for (i in 1:length(x1_grid)) {
mu <- sample$betaSample %*% c(1, x1_grid[i], 0.7, 1, 0, 0.5, 0)
eps <- rnorm(nIter, 0, sample$sigma2Sample)
mu + eps
}
hist(rnorm(nIter, 0, sample$sigma2Sample) + mu)
hist(rnorm(nIter, mu, sample$sigma2Sample))
quantile(y_sample)
y_sample <- mu + eps
quantile(y_sample)
?quantile
quantile(y_sample, probs=c(0.005, 0.995))
interval[1]
interval <- quantile(y_sample, probs=c(0.005, 0.995))
interval[1]
int_mat <- matrix(0, length(x1_grid), 2)
int_mat[i,] <- interval
int_mat
x1_grid <- seq(min(X[1:nrow(X), 2]), max(X[1:nrow(X), 2]), 0.01)
int_mat <- matrix(0, length(x1_grid), 2)
for (i in 1:length(x1_grid)) {
mu <- sample$betaSample %*% c(1, x1_grid[i], 0.7, 1, 0, 0.5, 0)
eps <- rnorm(nIter, 0, sample$sigma2Sample)
y_sample <- mu + eps
interval <- quantile(y_sample, probs=c(0.005, 0.995))
int_mat[i,] <- interval
}
plot(x1_grid, int_mat[, 1])
lines(x1_grid, int_mat[, 2])
int_mat
x1_grid <- seq(min(X[1:nrow(X), 2]), max(X[1:nrow(X), 2]), 0.01)
int_mat <- matrix(0, length(x1_grid), 2)
for (i in 1:length(x1_grid)) {
mu <- sample$betaSample %*% c(1, x1_grid[i], 0.7, 1, 0, 0.5, 0)
eps <- rnorm(nIter, 0, sample$sigma2Sample)
y_sample <- mu + eps
interval <- quantile(y_sample, probs=c(0.005, 0.995))
int_mat[i,] <- interval
}
plot(x1_grid, int_mat[, 1])
lines(x1_grid, int_mat[, 2])
?plot
x1_grid <- seq(min(X[1:nrow(X), 2]), max(X[1:nrow(X), 2]), 0.01)
int_mat <- matrix(0, length(x1_grid), 2)
for (i in 1:length(x1_grid)) {
mu <- sample$betaSample %*% c(1, x1_grid[i], 0.7, 1, 0, 0.5, 0)
eps <- rnorm(nIter, 0, sample$sigma2Sample)
y_sample <- mu + eps
interval <- quantile(y_sample, probs=c(0.005, 0.995))
int_mat[i,] <- interval
}
plot(x1_grid, int_mat[, 1], type="l")
lines(x1_grid, int_mat[, 2])
x1_grid <- seq(min(X[1:nrow(X), 2]), max(X[1:nrow(X), 2]), 0.01)
int_mat <- matrix(0, length(x1_grid), 2)
for (i in 1:length(x1_grid)) {
mu <- sample$betaSample %*% c(1, x1_grid[i], 0.7, 1, 0, 0.5, 0)
eps <- rnorm(nIter, 0, sample$sigma2Sample)
y_sample <- mu + eps
interval <- quantile(y_sample, probs=c(0.005, 0.995))
int_mat[i,] <- interval
}
plot(x1_grid, int_mat[, 1], type="l", ylim=c(-2, 2))
lines(x1_grid, int_mat[, 2])
x1_grid <- seq(min(X[1:nrow(X), 2]), max(X[1:nrow(X), 2]), 0.01)
int_mat <- matrix(0, length(x1_grid), 2)
for (i in 1:length(x1_grid)) {
mu <- sample$betaSample %*% c(1, x1_grid[i], 0.7, 1, 0, 0.5, 0)
eps <- rnorm(nIter, 0, sample$sigma2Sample)
y_sample <- mu + eps
interval <- quantile(y_sample, probs=c(0.005, 0.995))
int_mat[i,] <- interval
}
plot(x1_grid, int_mat[, 1], type="l", ylim=c(-2, 5))
lines(x1_grid, int_mat[, 2])
x1_grid <- seq(min(X[1:nrow(X), 2]), max(X[1:nrow(X), 2]), 0.01)
int_mat <- matrix(0, length(x1_grid), 2)
for (i in 1:length(x1_grid)) {
mu <- sample$betaSample %*% c(1, x1_grid[i], 0.7, 1, 0, 0.5, 0)
eps <- rnorm(nIter, 0, sample$sigma2Sample)
y_sample <- mu + eps
interval <- quantile(y_sample, probs=c(0.005, 0.995))
int_mat[i,] <- interval
}
plot(x1_grid, int_mat[, 1], type="l", ylim=c(-2, 5), main="99% posterior predictive intervals")
lines(x1_grid, int_mat[, 2])
x1_grid <- seq(min(X[1:nrow(X), 2]), max(X[1:nrow(X), 2]), 0.01)
int_mat <- matrix(0, length(x1_grid), 2)
for (i in 1:length(x1_grid)) {
mu <- sample$betaSample %*% c(1, x1_grid[i], 0.7, 1, 0, 0.5, 0)
eps <- rnorm(nIter, 0, sample$sigma2Sample)
y_sample <- mu + eps
interval <- quantile(y_sample, probs=c(0.005, 0.995))
int_mat[i,] <- interval
}
plot(x1_grid, int_mat[, 1], type="l", ylim=c(-2, 5), main="99% posterior predictive intervals", xlab="x1", ylab="y")
lines(x1_grid, int_mat[, 2])
# Define grid and matrix to store intervals
x1_grid <- seq(min(X[1:nrow(X), 2]), max(X[1:nrow(X), 2]), 0.01)
int_mat <- matrix(0, length(x1_grid), 2)
# Loop over values in the grid
for (i in 1:length(x1_grid)) {
# Compute and sample mu and epsilon of the normal distribution
mu <- sample$betaSample %*% c(1, x1_grid[i], 0.7, 1, 0, 0.5, 0)
eps <- rnorm(nIter, 0, sample$sigma2Sample)
# Compute the y sample
y_sample <- mu + eps
interval <- quantile(y_sample, probs=c(0.005, 0.995))
int_mat[i,] <- interval
}
# Plot the upper and lower limit of the intervals over x1
plot(x1_grid, int_mat[, 1], type="l", ylim=c(-2, 5), main="99% posterior predictive intervals", xlab="x1", ylab="y")
lines(x1_grid, int_mat[, 2])
?bern
?rbern
?dbern
rbeta(10000, s + 5, f + 13)
n <- 40
s <- 10
f <- n - s
rbeta(10000, s + 5, f + 13)
hist(rbeta(10000, s + 5, f + 13))
n <- 40
s <- 10
f <- n - s
hist(rbeta(10000, s + 5, f + 13))
abline(v=4/15)
sample <- rbeta(10000, s + 5, f + 13)
cat("Cost of buying insurance: ", 40 * (1 - theta) - 20 * theta)
- 20 * theta
# Without simulation
theta <- 4 / 15
- 20 * theta
* (1 - theta)
cat("Cost of buying insurance: ", 40 * (1 - theta) - 20 * theta)
cat("Gain of NOT buying insurance: ", 90 * (1 - theta) - 80 * theta)
cat("Gain of buying insurance: ", 40 * (1 - theta) - 20 * theta)
cat("Gain of NOT buying insurance: ", 90 * (1 - theta) - 80 * theta)
sample <- rbeta(10000, s + 5, f + 13)
sample
install.packages("xfun")
# Install required packages
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install("RBGL")
BiocManager::install("Rgraphviz")
BiocManager::install("gRain")
